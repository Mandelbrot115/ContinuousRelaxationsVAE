# -*- coding: utf-8 -*-
"""gaussianbernimprovedcleaner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14ERkGXNMSPrDshFzXU294EGILge08IWV
"""

import torch
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal, Bernoulli
from torchvision.utils import save_image

use_cuda=True
#parameters
learning_param = 0.0001
epochs = 125
batchsize = 32
image_dim = 784
hidden_layer1_dim = 512
hidden_layer2_dim = 256
hidden_layer3_dim = 128
latent_var_dim = 56

torch.set_grad_enabled(True)
train_set = torchvision.datasets.MNIST(root='./data/MNIST', train=True, download=True, 
                                              transform=transforms.Compose([transforms.ToTensor()]))
train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchsize)

class vae_Encoder(nn.Module):
    
    def __init__(self):
        super(vae_Encoder, self).__init__()
        self.hidden1 = nn.Linear(in_features=image_dim, out_features=hidden_layer1_dim)
        self.hidden2 = nn.Linear(hidden_layer1_dim, hidden_layer2_dim)
        self.hidden3 = nn.Linear(hidden_layer2_dim, hidden_layer3_dim)
        self.mean_layer = nn.Linear(hidden_layer3_dim, latent_var_dim) 
        self.logvar_layer = nn.Linear(hidden_layer3_dim, latent_var_dim)
    def forward(self, t): #t is input tensor
        #encoder hidden layers
        t = t.reshape(-1, image_dim)
        t = F.relu(self.hidden1(t))
        t = F.relu(self.hidden2(t))
        t = F.relu(self.hidden3(t))
        #mean and logvar
        tmean = self.mean_layer(t)
        tlogvar = self.logvar_layer(t)  
        return Normal(tmean, torch.exp(0.5*tlogvar))

class vae_Decoder(nn.Module):
    
    def __init__(self):
        super(vae_Decoder, self).__init__()
        self.hidden1 = nn.Linear(in_features=latent_var_dim, out_features=hidden_layer3_dim)
        self.hidden2 = nn.Linear(hidden_layer3_dim, hidden_layer2_dim)
        self.hidden3 = nn.Linear(hidden_layer2_dim, hidden_layer1_dim)
        self.reconstructed_image = nn.Linear(hidden_layer1_dim, image_dim) 
    def forward(self, t): #t is input tensor
        #encoder hidden layers
        t = F.relu(self.hidden1(t))
        t = F.relu(self.hidden2(t))
        t = F.relu(self.hidden3(t))
        #mean and logvar
        treconstructed = torch.sigmoid(self.reconstructed_image(t))
        return treconstructed

def vaeLoss(encoder_normal, decoder_bernoulli, xs, zs):
  
    log_likelihood = decoder_bernoulli.log_prob(xs).sum()
    prior = Normal(torch.tensor([0.0], device='cuda'), torch.tensor([1.0], device='cuda'))
    log_prior = prior.log_prob(zs).sum()
    #log_bernPrior = Bernoulli(torch.tensor([0.5], device='cuda')).log_prob(zs).sum()
    log_q_z = encoder_normal.log_prob(zs).sum()
    ELBO = log_likelihood + log_prior - log_q_z #aproximate expectation
    
    return -ELBO

vae_encoder = vae_Encoder()
vae_encoder.cuda()
vae_decoder = vae_Decoder()
vae_decoder.cuda()
optimiser = torch.optim.Adam([*vae_encoder.parameters(), *vae_decoder.parameters()], lr=learning_param)
#training
for epoch in range(epochs):
    for batch in train_loader:
        xs, labels = batch
        xs = xs.cuda()
        optimiser.zero_grad()
        encoder_normal = vae_encoder(xs)
        zs = encoder_normal.rsample()
        decoder_bernoulli = Bernoulli(vae_decoder(zs))
        xs = xs.reshape(batchsize,image_dim)
        loss = vaeLoss(encoder_normal, decoder_bernoulli, xs, zs)
        loss.backward()
        optimiser.step()
    print(epoch)
    print(loss)