# -*- coding: utf-8 -*-
"""StepLogi_cleaner.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dakqOBuwejgndRVcIBlzp-bTEtt0zUqV
"""

import torch
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal, Bernoulli, Uniform, TransformedDistribution, SigmoidTransform, AffineTransform
from torchvision.utils import save_image
from scipy import optimize

use_cuda=True
#parameters
learning_param = 0.0001
epochs = 30
batchsize = 32
image_dim = 784
hidden_layer1_dim = 512
hidden_layer2_dim = 256
hidden_layer3_dim = 128
latent_var_dim = 8
sigmoidSteepness = 10

torch.set_grad_enabled(True)
train_set = torchvision.datasets.MNIST(root='./data/MNIST', train=True, download=True,
                                              transform=transforms.Compose([transforms.ToTensor()]))
train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchsize)

def stepLogiInvCDF(u, alpha):
  A = u > (2-alpha)/2
  B = A.float()
  C = torch.log(u/(2-alpha-u)) #case1
  C[C != C] = 0
  D = torch.log((u + alpha - 1)/(1 - u)) #case2
  D[D != D] = 0
  samples = (1-B)*C + B*D
  return samples

class StepLogi:

    def __init__(self, alpha):
      self.alpha = alpha
    def log_prob(self, zs):
      #chof = torch.where(zs<0, torch.log(2-self.alpha), torch.log(self.alpha))
      #return -zs - 2*torch.log(1+torch.exp(-zs)) + chof
    def rsample(self):
      u = Uniform(1e-6, (1-1e-6)).rsample(self.alpha.shape).cuda()
      z = stepLogiInvCDF(u, self.alpha)
      return z

class Logistic:

    def __init__(self, mean, scale):
      self.mean = mean
      self.scale = scale

    def log_prob(self, zs):
      return (-(zs-self.mean)/self.scale) -torch.log(self.scale) - 2*torch.log(1+torch.exp(-(zs-self.mean)/self.scale))
    def rsample(self):
      eps = Uniform(1e-6, (1-1e-6)).rsample(self.mean.shape)
      smpl = torch.log(eps/(1-eps)).cuda()
      return self.mean + self.scale * smpl

class vae_Encoder(nn.Module):

    def __init__(self):
        super(vae_Encoder, self).__init__()
        self.hidden1 = nn.Linear(in_features=image_dim, out_features=hidden_layer1_dim)
        self.hidden2 = nn.Linear(hidden_layer1_dim, hidden_layer2_dim)
        self.hidden3 = nn.Linear(hidden_layer2_dim, hidden_layer3_dim)
        self.alpha_layer = nn.Linear(hidden_layer3_dim, latent_var_dim)
    def forward(self, x): #t is input tensor
        #encoder hidden layers
        x = x.reshape(-1, image_dim)
        x = F.relu(self.hidden1(x))
        x = F.relu(self.hidden2(x))
        x = F.relu(self.hidden3(x))
        #mean and logvar
        alpha = 2*torch.sigmoid(self.alpha_layer(x))
        return alpha

class vae_Decoder(nn.Module):

    def __init__(self):
        super(vae_Decoder, self).__init__()
        self.hidden1 = nn.Linear(in_features=latent_var_dim, out_features=hidden_layer3_dim)
        self.hidden2 = nn.Linear(hidden_layer3_dim, hidden_layer2_dim)
        self.hidden3 = nn.Linear(hidden_layer2_dim, hidden_layer1_dim)
        self.reconstructed_image = nn.Linear(hidden_layer1_dim, image_dim)
    def forward(self, z): #t is input tensor
        #encoder hidden layers
        z = F.relu(self.hidden1(z))
        z = F.relu(self.hidden2(z))
        z = F.relu(self.hidden3(z))
        treconstructed = torch.sigmoid(self.reconstructed_image(z))
        #print("reconss ", treconstructed)
        return treconstructed

def stepLogiLoss(alphaL, decoder_bernoulli, xs):
  log_likelihood = decoder_bernoulli.log_prob(xs).sum()
  posterior_priorKL = 0.5*(2-alphaL)*torch.log(2-alphaL + 1e-5) + 0.5*alphaL*torch.log(alphaL + 1e-5)
  ELBO = log_likelihood.sum() - posterior_priorKL.sum() #aproximate expectation
  return -ELBO

#creating the encoder and decoder to be trained
vae_encoder = vae_Encoder()
vae_encoder.cuda()
vae_decoder = vae_Decoder()
vae_decoder.cuda()

optimiser = torch.optim.Adam([*vae_encoder.parameters(), *vae_decoder.parameters()], lr=learning_param)

for epoch in range(epochs):
  for batch in train_loader:

    xs, labels = batch
    xs = xs.cuda()
    optimiser.zero_grad()
    #Encoder ouput is the StepLogistic defining parameter alpha
    alpha = vae_encoder(xs)
    #Defining the StepLogistic approximate posterior using alpha
    encoder_stepLogi = StepLogi(alpha)
    zs = encoder_stepLogi.rsample()
    #Decoder output is the normalised pixel values of the generated image
    decoder_bernoulli = Bernoulli(vae_decoder(torch.sigmoid(sigmoidSteepness*zs)))
    xs = xs.reshape(batchsize,image_dim)
    #Calculating the loss value
    loss = stepLogiLoss(alpha, decoder_bernoulli, xs)
    #Optimizing over the loss
    loss.backward()
    optimiser.step()

  print(epoch)
  #Training complete. We can generate new, random images by sampling from a Bernoulli distributions
  #with dimension "latent_var_dim", and passing this sample through the decoder network.
